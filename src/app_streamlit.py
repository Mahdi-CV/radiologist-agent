import asyncio
import streamlit as st
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.messages import (
    PartDeltaEvent, TextPartDelta,
    FunctionToolCallEvent, FunctionToolResultEvent,
)
from config import orch_model
from tools_orchestrator import analyse_image_base64, show_reference_images_tool
from pathlib import Path

DATA_DIR = Path(__file__).parent / "data"
DATA_DIR.mkdir(exist_ok=True)
SYSTEM_PROMPT = """
    ROLE
    You are a radiology assistant.

    TOOLS
    1. analyse_image_base64(path:str)            -> RadiologyReport
    2. show_reference_images_tool(path:str)      -> dict
    3. send_email(to:list[str], subject:str,
                body:str, cc:list[str]|None=[],
                bcc:list[str]|None=[])         -> Gmail message ID

    FLOW
    A) Get image
    ‚Ä¢ If no image path: ask
        ‚ÄúWhich image file should I analyse? (default = data/image.jpg)‚Äù

    B) Analyse image
    ‚Ä¢ Once you have <PATH> call:
        {"name":"analyse_image_base64","arguments":{"path":"<PATH>"}}

    C) Present result
    ‚Ä¢ Tool returns {critical, diagnosis_description, clinical_recommendations}
    ‚Ä¢ Show this summary:
        Diagnosis: <diagnosis_description>
        Recommendations: <clinical_recommendations>
        Critical: Yes/No

    D) Offer reference images  ‚îÄ‚îÄ ALWAYS happens first
    ‚Ä¢ Ask exactly:
        ‚ÄúWould you like to view reference images from similar confirmed cases?‚Äù
    ‚Ä¢ If user replies yes/yep/‚Äúshow them‚Äù‚Ä¶ IMMEDIATELY call:
        {"name":"show_reference_images_tool","arguments":{"confirm":"yes"}}
        Say nothing else.
    ‚Ä¢ After the tool finishes ‚Äîor if the user said ‚Äúno‚Äù ‚Äîcontinue to step E.

    E) Handle critical cases  ‚îÄ‚îÄ ONLY if critical == true
    ‚Ä¢ Ask:
        ‚ÄúThis case is marked as critical. Would you like to send it for peer review?‚Äù
    ‚Ä¢ If user agrees:
        1. Draft a plain-text email (from Dr. Mahdi Ghodsi) with the summary.
        2. Show the draft and ask:
            ‚ÄúWould you like me to send this email?‚Äù
    ‚Ä¢ If user confirms sending:
        1. Ask for recipient address.
        2. Call IMMEDIATELY:
            {"name":"send_email",
            "arguments":{"to":["<EMAIL>"],"subject":"<SUBJECT>","body":"<BODY>"}}

    RULES
    ‚Ä¢ Never skip a step.
    ‚Ä¢ D must finish (or be declined) before E begins.
    ‚Ä¢ Never send an email without explicit confirmation.
    ‚Ä¢ Never invent clinical data.
    """.strip()

async def build_orchestrator():
    
    gmail_server = MCPServerStdio(
        command="npx",
        args=["-y", "@gongrzhe/server-gmail-autoauth-mcp"],
    )

    system_prompt = SYSTEM_PROMPT

    return Agent(
        model=orch_model,
        mcp_servers=[gmail_server],
        tools=[analyse_image_base64, show_reference_images_tool],
        system_prompt=system_prompt,
        instrument=True,
    )
    

class StreamlitChatUI:
    def __init__(self):
        self.orchestrator = None
        if "messages" not in st.session_state:
            st.session_state.messages = []
        if "internal_history" not in st.session_state:
            st.session_state.internal_history = []

    async def initialize(self):
        if self.orchestrator is None:
            print("‚è≥ Bootstrapping orchestrator...")
            with st.spinner("Initializing orchestrator..."):
                self.orchestrator = await build_orchestrator()
            print("‚úÖ Orchestrator ready!")
        return self.orchestrator
    
    async def process_message(self, message: str):
        await self.initialize()

        # Record the user‚Äôs turn in the UI (you already do this)
        st.session_state.messages.append({"role": "user", "content": message})

        assistant_placeholder = st.empty()
        full_response = ""

        async with self.orchestrator.run_mcp_servers():
            # Pass in the prior turns (ModelMessage list) exactly as your CLI does
            async with self.orchestrator.iter(
                user_prompt=message,
                message_history=st.session_state.internal_history,  # this holds ModelMessage objects
            ) as run:

                async for node in run:
                    if Agent.is_call_tools_node(node):
                        async with node.stream(run.ctx) as s:
                            async for ev in s:
                                if isinstance(ev, FunctionToolCallEvent):
                                    call = f"\n‚ñ∂Ô∏è Tool call ‚Üí {ev.part.tool_name}{ev.part.args}\n"
                                    full_response += call
                                    assistant_placeholder.markdown(full_response)
                                    print(call.strip())  
                                elif isinstance(ev, FunctionToolResultEvent):
                                    result_str = f"‚úÖ Tool Result:\n{ev.result}\n\n"
                                    full_response += result_str
                                    assistant_placeholder.markdown(full_response)
                                    print(result_str.strip())
                                    if ev.result.tool_name == "show_reference_images_tool":
                                        print("‚úÖ show_reference_images_tool was triggered!")
                                        self.show_reference_images()


                    elif Agent.is_model_request_node(node):
                        async with node.stream(run.ctx) as s:
                            async for ev in s:

                                if isinstance(ev, PartDeltaEvent) and isinstance(ev.delta, TextPartDelta):
                                    #print(ev.delta.content_delta, end="", flush=True)
                                    full_response += ev.delta.content_delta
                                    assistant_placeholder.markdown(full_response)

            
            new_messages = run.result.all_messages()
            st.session_state.internal_history.extend(new_messages) 

        # And update the UI chat log
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        assistant_placeholder.markdown(full_response)
    
    def show_reference_images(self):
        print("we got here!!!!")
        # st.markdown("### ü©ª Reference Cases")
        st.markdown("These are sample cases with confirmed diagnoses:")

        cols = st.columns(4)
        image_paths = [
            "data/cancer/sample_1.jpg",
            "data/cancer/sample_2.jpg",
            "data/cancer/sample_3.jpg",
            "data/cancer/sample_4.jpg",
        ]
        for col, path in zip(cols, image_paths):
            with col:
                st.image(path, use_container_width=True, caption=path.split("/")[-1].replace("_", " ").title())



def main():
    st.title("üè• Radiology Assistant")
    st.markdown("Analyze medical images and manage communications")
    
    ui = StreamlitChatUI()
    
    # File upload
    uploaded_file = st.file_uploader("Upload medical image", type=["jpg", "png"])
    if uploaded_file and not st.session_state.get("image_uploaded"):
        # 1‚É£  Persist inside DATA_DIR (radiology_async/data/)
        save_path = DATA_DIR / uploaded_file.name
        with open(save_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        st.image(uploaded_file, caption="Uploaded Image", use_container_width=True)

        # 2‚É£  Analyse it ‚Äì pass the *relative* path visible to the MCP FS server
        st.session_state.image_uploaded = True
        asyncio.run(
            ui.process_message(f"Analyze this image: data/{uploaded_file.name}")
        )

    
    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    # Chat input
    if prompt := st.chat_input("Enter your message..."):
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            asyncio.run(ui.process_message(prompt))

if __name__ == "__main__":
    main()